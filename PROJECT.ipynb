{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "given size of the data 23486\n",
      " size of data after removing repitations 23486\n",
      "start of pre_processing\n",
      "0          absolutely wonderful silky sexy comfortable\n",
      "1    love dress sooo pretty happened find store gla...\n",
      "2    high hope dress really wanted work initially o...\n",
      "3    love love love jumpsuit fun flirty fabulous ev...\n",
      "4    shirt flattering due adjustable front tie perf...\n",
      "Name: processed_tweets, dtype: object\n",
      "0.000000    937\n",
      "0.500000    152\n",
      "0.250000    113\n",
      "0.300000     92\n",
      "0.450000     75\n",
      "           ... \n",
      "0.294697      1\n",
      "0.164198      1\n",
      "0.371561      1\n",
      "0.080301      1\n",
      "0.271701      1\n",
      "Name: sentiment, Length: 14239, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from  sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "#directly downloads packages for required things\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')\n",
    "from nltk.corpus import stopwords\n",
    "import  pandas as pd\n",
    "import string\n",
    "#importing the data\n",
    "from collections import Counter\n",
    "#pass the test dataset here\n",
    "test_data=pd.read_csv(r'ecommerce.csv',engine='python')\n",
    "print(\"given size of the data\",len(test_data))\n",
    "#dropping the null values\n",
    "test_data=test_data.drop_duplicates()\n",
    "print(\" size of data after removing repitations\",len(test_data))\n",
    "#print(test_data['TWEETS'].value_counts())\n",
    "#DATA PRECPROCESSING STARTS FROM HERRE\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def normalizer(tweet):\n",
    "    aplhabets = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    tokens = nltk.word_tokenize(aplhabets)\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    processed_words = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in processed_words]\n",
    "    return lemmas\n",
    "\n",
    "#create a coloum in the dataset to get only normalized words\n",
    "print(\"start of pre_processing\")\n",
    "test_data[\"Review Text\"]=test_data[\"Review Text\"].astype(str)\n",
    "test_data[\"normalized_Tweets\"]=np.nan\n",
    "test_data[\"normalized_Tweets\"]=test_data[\"Review Text\"].apply(lambda x: normalizer(x))\n",
    "#now we are converting to ngrams to get the meanining\n",
    "\n",
    "test_data[\"processed_tweets\"]=np.nan\n",
    "test_data[\"sentiment\"]=np.nan\n",
    "\n",
    "\n",
    "def sentiment(tweet):\n",
    "    sentiment=TextBlob(tweet)\n",
    "    \n",
    "    \n",
    "    return sentiment.sentiment.polarity\n",
    "#note 1=en,2=n,3=nue,4=p,5=exp\n",
    "def feature_labelling(score):\n",
    "    if score>=-1 and score<-0.5:\n",
    "        return 1\n",
    "    elif score>=-0.5 and score<-0.1:\n",
    "        return 2\n",
    "    elif score>=-0.1 and score<0.2:\n",
    "        return 3\n",
    "    elif score>=0.2 and score<0.6:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "test_data[\"processed_tweets\"]=test_data[\"normalized_Tweets\"].apply(lambda x: \" \".join(list(x)))\n",
    "print(test_data[\"processed_tweets\"].head())\n",
    "\n",
    "test_data[\"sentiment\"]=test_data[\"Review Text\"].apply(lambda x: sentiment(x))\n",
    "print(test_data[\"sentiment\"].value_counts())\n",
    "test_data[\"Rating2\"]=np.nan\n",
    "test_data[\"Rating2\"]=test_data[\"sentiment\"].apply(lambda x: feature_labelling(x))\n",
    "#print(\"displaying the count of data based on caluculated rating\")\n",
    "#print(test_data[\"Target_label\"].value_counts())\n",
    "test_data.to_csv('.\\preprocessed_reviews2.csv')\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#final_df=test_data[['sentiment','Rating']]\n",
    "#print(test_data['sentiment'].value_counts())\n",
    "#print(test_data['Target_label'].value_counts())\n",
    "#train , test = train_test_split(final_df, test_size = 0.15)\n",
    "#x_train = train.drop('Rating', axis=1)\n",
    "\n",
    "#y_train = train['Rating']\n",
    "\n",
    "#x_test = test.drop('Rating', axis = 1)\n",
    "#y_test = test['Rating']\n",
    "\n",
    "\n",
    "#clf1 =  RandomForestClassifier(n_estimators=5000,max_depth=1)\n",
    "\n",
    "\n",
    "#coding testing and saving with svc    \n",
    "#Train the model using the training sets\n",
    "#clf1.fit(x_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "#y_pred1 = clf1.predict(x_test)\n",
    "#print(y_pred)\n",
    "#saving predictions\n",
    "\n",
    "\n",
    "#from sklearn.metrics import f1_score\n",
    "#print(\"ramdom forest classcification model accuracy(in %):\",metrics.accuracy_score(y_test, y_pred1) * 100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000    937\n",
      "0.500000    152\n",
      "0.250000    113\n",
      "0.300000     92\n",
      "0.450000     75\n",
      "           ... \n",
      "0.294697      1\n",
      "0.164198      1\n",
      "0.371561      1\n",
      "0.080301      1\n",
      "0.271701      1\n",
      "Name: sentiment, Length: 14239, dtype: int64\n",
      "4    12893\n",
      "3     9371\n",
      "5      797\n",
      "2      414\n",
      "1       11\n",
      "Name: Rating2, dtype: int64\n",
      "[21:23:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishna\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost classcification model accuracy(in %): 99.80130570536475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "final_df=test_data[['sentiment','Rating2']]\n",
    "print(test_data['sentiment'].value_counts())\n",
    "print(test_data['Rating2'].value_counts())\n",
    "train , test = train_test_split(final_df, test_size = 0.3)\n",
    "x_train = train.drop('Rating2', axis=1)\n",
    "\n",
    "y_train = train['Rating2']\n",
    "\n",
    "x_test = test.drop('Rating2', axis = 1)\n",
    "y_test = test['Rating2']\n",
    "clf2 = XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf2.fit(x_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred2 = clf2.predict(x_test)\n",
    "#print(y_pred)\n",
    "#saving predictions\n",
    "\n",
    "print(\"xgboost classcification model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred2) * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a sentencenot bad\n",
      "[0.3499999999999999]\n",
      "[4]\n",
      "enter a sentence,q to exitq\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_Input=input(\"enter a sentence\")\n",
    "while sample_Input!='q':\n",
    " #sample_Input=input(\"enter a sentence\")\n",
    " n_sample=normalizer(sample_Input)\n",
    " \n",
    " processed_string=' '.join(n_sample)\n",
    " if processed_string!= None:\n",
    "    if \"not\" in sample_Input:\n",
    "        score=sentiment(processed_string)*-0.5\n",
    "    else:\n",
    "        score=sentiment(processed_string)\n",
    " else:\n",
    "    print(\"input is null\")\n",
    " print([score])\n",
    " ypd=clf2.predict(np.asarray([score]))\n",
    " print(ypd)\n",
    " sample_Input = input(\"enter a sentence,q to exit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
