{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Krishna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "given size of the data 61267\n",
      " size of data after removing repitations 18543\n",
      "start of pre_processing\n",
      "displaying the count of data based on caluculated rating\n",
      "3    10656\n",
      "4     6536\n",
      "2      767\n",
      "5      516\n",
      "1       68\n",
      "Name: Target_label, dtype: int64\n",
      "ramdom forest classcification model accuracy(in %): 92.70309130122214\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from  sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import textblob\n",
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "#directly downloads packages for required things\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')\n",
    "from nltk.corpus import stopwords\n",
    "import  pandas as pd\n",
    "import string\n",
    "#importing the data\n",
    "from collections import Counter\n",
    "#pass the test dataset here\n",
    "test_data=pd.read_csv(r'data2.csv',engine='python')\n",
    "print(\"given size of the data\",len(test_data))\n",
    "#dropping the null values\n",
    "test_data=test_data.drop_duplicates()\n",
    "print(\" size of data after removing repitations\",len(test_data))\n",
    "#print(test_data['TWEETS'].value_counts())\n",
    "#DATA PRECPROCESSING STARTS FROM HERRE\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def normalizer(tweet):\n",
    "    aplhabets = re.sub(\"[^a-zA-Z]\", \" \",tweet)\n",
    "    tokens = nltk.word_tokenize(aplhabets)[2:]\n",
    "    lower_case = [l.lower() for l in tokens]\n",
    "    processed_words = list(filter(lambda l: l not in stop_words, lower_case))\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in processed_words]\n",
    "    return lemmas\n",
    "\n",
    "#create a coloum in the dataset to get only normalized words\n",
    "print(\"start of pre_processing\")\n",
    "test_data[\"Review Text\"]=test_data[\"Review Text\"].astype(str)\n",
    "test_data[\"normalized_Tweets\"]=np.nan\n",
    "test_data[\"normalized_Tweets\"]=test_data[\"Review Text\"].apply(lambda x: normalizer(x))\n",
    "#now we are converting to ngrams to get the meanining\n",
    "\n",
    "test_data[\"processed_tweets\"]=np.nan\n",
    "test_data[\"sentiment\"]=np.nan\n",
    "def sentiment(tweet):\n",
    "    return textblob.TextBlob(tweet).sentiment.polarity\n",
    "def feauture_labelling(score):\n",
    "    if (score >= 1):\n",
    "        return 5\n",
    "\n",
    "    elif (score <= -1):\n",
    "        return 1\n",
    "\n",
    "    elif ((score > -0.5) and (score < 0.5)):\n",
    "        return 3\n",
    "\n",
    "    elif ((score >= 0.5) and (score < 1)):\n",
    "        return 4\n",
    "\n",
    "    elif ((score <= -0.5) and (score > -1)):\n",
    "        return 2\n",
    "#note 1=en,2=n,3=nue,4=p,5=exp\n",
    "\n",
    "\n",
    "test_data[\"processed_tweets\"]=test_data[\"normalized_Tweets\"].apply(lambda x: \" \".join(list(x)))\n",
    "#print(test_data[\"processed_tweets\"].head())\n",
    "\n",
    "test_data[\"sentiment\"]=test_data[\"processed_tweets\"].apply(lambda x: sentiment(x))\n",
    "#print(test_data[\"sentiment\"].value_counts())\n",
    "test_data[\"Target_label\"]=np.nan\n",
    "test_data[\"Target_label\"]=test_data[\"sentiment\"].apply(lambda x: feauture_labelling(x))\n",
    "print(\"displaying the count of data based on caluculated rating\")\n",
    "print(test_data[\"Target_label\"].value_counts())\n",
    "test_data.to_csv('.\\preprocessed_reviews2.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "final_df=test_data[['sentiment','Target_label']]\n",
    "#print(test_data['sentiment'].value_counts())\n",
    "#print(test_data['Target_label'].value_counts())\n",
    "train , test = train_test_split(final_df, test_size = 0.15)\n",
    "x_train = train.drop('Target_label', axis=1)\n",
    "\n",
    "y_train = train['Target_label']\n",
    "\n",
    "x_test = test.drop('Target_label', axis = 1)\n",
    "y_test = test['Target_label']\n",
    "\n",
    "\n",
    "clf1 =  RandomForestClassifier(n_estimators=5000,max_depth=1)\n",
    "\n",
    "\n",
    "#coding testing and saving with svc    \n",
    "#Train the model using the training sets\n",
    "clf1.fit(x_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred1 = clf1.predict(x_test)\n",
    "#print(y_pred)\n",
    "#saving predictions\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"ramdom forest classcification model accuracy(in %):\",metrics.accuracy_score(y_test, y_pred1) * 100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.000000    5934\n",
      " 0.500000    2323\n",
      " 0.800000     925\n",
      " 0.600000     646\n",
      " 0.700000     594\n",
      "             ... \n",
      "-0.053788       1\n",
      " 0.642500       1\n",
      " 0.359184       1\n",
      " 0.187216       1\n",
      "-0.012500       1\n",
      "Name: sentiment, Length: 1912, dtype: int64\n",
      "3    10656\n",
      "4     6536\n",
      "2      767\n",
      "5      516\n",
      "1       68\n",
      "Name: Target_label, dtype: int64\n",
      "[14:09:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krishna\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost classcification model accuracy(in %): 100.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "final_df=test_data[['sentiment','Target_label']]\n",
    "print(test_data['sentiment'].value_counts())\n",
    "print(test_data['Target_label'].value_counts())\n",
    "train , test = train_test_split(final_df, test_size = 0.3)\n",
    "x_train = train.drop('Target_label', axis=1)\n",
    "\n",
    "y_train = train['Target_label']\n",
    "\n",
    "x_test = test.drop('Target_label', axis = 1)\n",
    "y_test = test['Target_label']\n",
    "clf2 = XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf2.fit(x_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred2 = clf2.predict(x_test)\n",
    "#print(y_pred)\n",
    "#saving predictions\n",
    "\n",
    "print(\"xgboost classcification model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred2) * 100)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a sentenceq\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_Input=input(\"enter a sentence\")\n",
    "while sample_Input!='q':\n",
    " #sample_Input=input(\"enter a sentence\")\n",
    " n_sample=normalizer(sample_Input)\n",
    " \n",
    " processed_string=' '.join(n_sample)\n",
    "    \n",
    " score=sentiment(processed_string)\n",
    " #print(np.asarray([score]))\n",
    " ypd=clf2.predict(np.asarray([score]))\n",
    " print(ypd)\n",
    " sample_Input = input(\"enter a sentence,q to exit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
